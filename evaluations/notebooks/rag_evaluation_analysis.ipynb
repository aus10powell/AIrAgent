{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Fix import path (same as dataset creation)\n",
    "project_root = os.path.abspath('../..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from evaluations.runners.rag_evaluator import RAGEvaluator\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: evaluations/results/rag_evaluation\n",
      "Using dataset files:\n",
      "üìÅ Chunks: ../datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "üìÅ Queries: ../datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n",
      "\n",
      "Dataset info:\n",
      "  Total chunks: 250\n",
      "  Total queries: 596\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator()\n",
    "print(f\"Results will be saved to: {evaluator.results_dir}\")\n",
    "\n",
    "# Find the most recent dataset files\n",
    "dataset_dir = Path(\"../datasets/rag_evaluation\")\n",
    "chunks_files = list(dataset_dir.glob(\"chunks_*.json\"))\n",
    "queries_files = list(dataset_dir.glob(\"queries_*.json\"))\n",
    "\n",
    "if chunks_files and queries_files:\n",
    "    # Use the most recent files (by filename)\n",
    "    chunks_file = str(sorted(chunks_files)[-1])\n",
    "    queries_file = str(sorted(queries_files)[-1])\n",
    "    \n",
    "    print(f\"Using dataset files:\")\n",
    "    print(f\"üìÅ Chunks: {chunks_file}\")\n",
    "    print(f\"üìÅ Queries: {queries_file}\")\n",
    "    \n",
    "    # Load to check dataset size\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    with open(queries_file, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "        \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Total chunks: {len(chunks_data)}\")\n",
    "    print(f\"  Total queries: {len(queries_data)}\")\n",
    "else:\n",
    "    print(\"‚ùå No dataset files found. Please run dataset_creation.ipynb first.\")\n",
    "    chunks_file = None\n",
    "    queries_file = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
