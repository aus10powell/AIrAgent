{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n",
      "📁 Results directory: evaluations/results/rag_evaluation\n",
      "\n",
      "Using dataset files:\n",
      "📁 Chunks: ../datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "📁 Queries: ../datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n",
      "\n",
      "Dataset info:\n",
      "  Total chunks: 250\n",
      "  Total queries: 596\n",
      "🧠 use_llm_judge parameter available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Fix import path (same as before)\n",
    "project_root = os.path.abspath('../..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from evaluations.runners.rag_evaluator import RAGEvaluator\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✅ Imports successful!\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator()\n",
    "print(f\"📁 Results directory: {evaluator.results_dir}\")\n",
    "\n",
    "# Find the most recent dataset files\n",
    "dataset_dir = Path(\"../datasets/rag_evaluation\")\n",
    "chunks_files = list(dataset_dir.glob(\"chunks_*.json\"))\n",
    "queries_files = list(dataset_dir.glob(\"queries_*.json\"))\n",
    "\n",
    "if chunks_files and queries_files:\n",
    "    # Use the most recent files (by filename)\n",
    "    chunks_file = str(sorted(chunks_files)[-1])\n",
    "    queries_file = str(sorted(queries_files)[-1])\n",
    "    \n",
    "    print(f\"\\nUsing dataset files:\")\n",
    "    print(f\"📁 Chunks: {chunks_file}\")\n",
    "    print(f\"📁 Queries: {queries_file}\")\n",
    "    \n",
    "    # Load to check dataset size\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    with open(queries_file, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "        \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Total chunks: {len(chunks_data)}\")\n",
    "    print(f\"  Total queries: {len(queries_data)}\")\n",
    "    \n",
    "    # Check if the new parameter exists\n",
    "    import inspect\n",
    "    sig = inspect.signature(RAGEvaluator.run_evaluation)\n",
    "    if 'use_llm_judge' in sig.parameters:\n",
    "        print(\"🧠 use_llm_judge parameter available!\")\n",
    "    else:\n",
    "        print(\"❌ use_llm_judge parameter NOT found\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No dataset files found. Please run dataset_creation.ipynb first.\")\n",
    "    chunks_file = None\n",
    "    queries_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: evaluations/results/rag_evaluation\n",
      "Using dataset files:\n",
      "📁 Chunks: ../datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "📁 Queries: ../datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n",
      "\n",
      "Dataset info:\n",
      "  Total chunks: 250\n",
      "  Total queries: 596\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator()\n",
    "print(f\"Results will be saved to: {evaluator.results_dir}\")\n",
    "\n",
    "# Find the most recent dataset files\n",
    "dataset_dir = Path(\"../datasets/rag_evaluation\")\n",
    "chunks_files = list(dataset_dir.glob(\"chunks_*.json\"))\n",
    "queries_files = list(dataset_dir.glob(\"queries_*.json\"))\n",
    "\n",
    "if chunks_files and queries_files:\n",
    "    # Use the most recent files (by filename)\n",
    "    chunks_file = str(sorted(chunks_files)[-1])\n",
    "    queries_file = str(sorted(queries_files)[-1])\n",
    "    \n",
    "    print(f\"Using dataset files:\")\n",
    "    print(f\"📁 Chunks: {chunks_file}\")\n",
    "    print(f\"📁 Queries: {queries_file}\")\n",
    "    \n",
    "    # Load to check dataset size\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    with open(queries_file, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "        \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Total chunks: {len(chunks_data)}\")\n",
    "    print(f\"  Total queries: {len(queries_data)}\")\n",
    "else:\n",
    "    print(\"❌ No dataset files found. Please run dataset_creation.ipynb first.\")\n",
    "    chunks_file = None\n",
    "    queries_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running test evaluation (5 queries)...\n",
      "Loading evaluation dataset...\n",
      "Evaluating 5 queries...\n",
      "Processing query 1/5: What type of property is this?...\n",
      "Processing query 2/5: How many bedrooms does this place have?...\n",
      "Processing query 3/5: What amenities are available?...\n",
      "Processing query 4/5: What neighborhood is this located in?...\n",
      "Processing query 5/5: How many people can this accommodate?...\n",
      "\n",
      "Evaluation completed!\n",
      "Results saved to: evaluations/results/rag_evaluation/rag_evaluation_20250529_212804.json\n",
      "Summary statistics:\n",
      "  total_queries: 5\n",
      "  successful_evaluations: 5\n",
      "  error_rate: 0.000\n",
      "  avg_response_length: 364.600\n",
      "  contains_expected_rate: 0.000\n",
      "  avg_word_overlap_score: 0.073\n",
      "  error_response_rate: 0.000\n",
      "  empty_response_rate: 0.000\n",
      "\n",
      "✅ Test evaluation completed!\n",
      "📊 Results saved to: evaluations/results/rag_evaluation/rag_evaluation_20250529_212804.json\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on a small subset first (5 queries for quick testing)\n",
    "if chunks_file and queries_file:\n",
    "    print(\"🧪 Running test evaluation (5 queries)...\")\n",
    "    \n",
    "    test_results_file = evaluator.run_evaluation(\n",
    "        chunks_file=chunks_file,\n",
    "        queries_file=queries_file,\n",
    "        chunk_size=100,\n",
    "        max_queries=5  # Small test run\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Test evaluation completed!\")\n",
    "    print(f\"📊 Results saved to: {test_results_file}\")\n",
    "else:\n",
    "    print(\"❌ Cannot run evaluation without dataset files.\")\n",
    "    test_results_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 EVALUATION SUMMARY:\n",
      "==================================================\n",
      "total_queries            : 5\n",
      "successful_evaluations   : 5\n",
      "error_rate               : 0.000\n",
      "avg_response_length      : 364.600\n",
      "contains_expected_rate   : 0.000\n",
      "avg_word_overlap_score   : 0.073\n",
      "error_response_rate      : 0.000\n",
      "empty_response_rate      : 0.000\n",
      "\n",
      "⏱️  Runtime: 9.49 seconds\n",
      "🔧 Chunk size: 100 words\n"
     ]
    }
   ],
   "source": [
    "if test_results_file:\n",
    "    # Load results\n",
    "    with open(test_results_file, 'r') as f:\n",
    "        evaluation_data = json.load(f)\n",
    "    \n",
    "    metadata = evaluation_data['metadata']\n",
    "    summary = evaluation_data['summary']\n",
    "    results = evaluation_data['results']\n",
    "    \n",
    "    print(\"📋 EVALUATION SUMMARY:\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:25}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{key:25}: {value}\")\n",
    "    \n",
    "    print(f\"\\n⏱️  Runtime: {metadata['total_runtime_seconds']:.2f} seconds\")\n",
    "    print(f\"🔧 Chunk size: {metadata['chunk_size']} words\")\n",
    "else:\n",
    "    print(\"❌ No results to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 SAMPLE RAG RESPONSES:\n",
      "================================================================================\n",
      "\n",
      "📝 EXAMPLE 1:\n",
      "❓ Query: What type of property is this?\n",
      "🎯 Expected: Entire guest suite\n",
      "🤖 RAG Answer: Based on the description, it appears that this is a basement suite or an in-law suite, likely located in a single-family home. The mention of \"2 Huge Private Rooms\" and \"Common Area\" suggests that the property has multiple living spaces, which is consistent with a multi-unit dwelling such as a duplex or a house with an annex.\n",
      "📊 Word Overlap: 0.333\n",
      "✅ Contains Expected: False\n",
      "📂 Category: property_details\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 EXAMPLE 2:\n",
      "❓ Query: How many bedrooms does this place have?\n",
      "🎯 Expected: 2.0\n",
      "🤖 RAG Answer: This listing doesn't explicitly mention the number of bedrooms, but it mentions \"2 Huge Private Rooms\". This suggests that there are at least two private rooms, likely bedrooms. However, without further information, we can't confirm the exact total number of bedrooms in the entire space.\n",
      "📊 Word Overlap: 0.000\n",
      "✅ Contains Expected: False\n",
      "📂 Category: property_details\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📝 EXAMPLE 3:\n",
      "❓ Query: What amenities are available?\n",
      "🎯 Expected: [\"Extra pillows and blankets\", \"Paid dryer \\u2013 In unit\", \"Conditioner\", \"Cooking basics\", \"Bed linens\", \"Clothing storage: wardrobe and dresser\", \"Shared patio or balcony\", \"Coffee maker\", \"Room-darkening shades\", \"Smart lock\", \"Books and reading material\", \"Portable fans\", \"Free parking on premises\", \"Self check-in\", \"Essentials\", \"Heating\", \"Children\\u2019s books and toys for ages 2-5 years old, 5-10 years old, and 10+ years old\", \"1-burner induction stovetop induction stove\", \"Barbecue utensils\", \"Outdoor furniture\", \"Smoke alarm\", \"Iron\", \"Coffee\", \"BBQ grill: gas\", \"Fire extinguisher\", \"Carbon monoxide alarm\", \"Mini fridge\", \"Luggage dropoff allowed\", \"Exterior security cameras on property\", \"Hot water kettle\", \"Record player\", \"55 inch HDTV with Amazon Prime Video, Apple TV, Disney+, HBO Max, Netflix\", \"Wine glasses\", \"Dishes and silverware\", \"Sun loungers\", \"Kitchenette\", \"Drying rack for clothing\", \"Board games\", \"Hangers\", \"Hot water\", \"Wifi\", \"Refrigerator\", \"Free street parking\", \"Sound system with Bluetooth and aux\", \"Private entrance\", \"Hair dryer\", \"Shower gel\", \"Outdoor dining area\", \"Toaster\", \"First aid kit\", \"Paid washer \\u2013 In unit\", \"Microwave\", \"Dining table\", \"Shampoo\"]\n",
      "🤖 RAG Answer: Based on the provided description, the following amenities are mentioned as being available:\n",
      "\n",
      "1. Private entry\n",
      "2. A common area (presumably for relaxation or socializing)\n",
      " \n",
      "Additional amenities that can be inferred but not explicitly stated include access to basic necessities like beds and possibly a bathroom, and possibly some form of entertainment or leisure facilities such as TV or internet connectivity.\n",
      "📊 Word Overlap: 0.029\n",
      "✅ Contains Expected: False\n",
      "📂 Category: amenities\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if test_results_file and results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Filter out error cases\n",
    "    successful_results = df_results[~df_results['actual_answer'].str.contains('Error', na=False)]\n",
    "    \n",
    "    if len(successful_results) > 0:\n",
    "        print(\"🔍 SAMPLE RAG RESPONSES:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (_, result) in enumerate(successful_results.head(3).iterrows()):\n",
    "            print(f\"\\n📝 EXAMPLE {i+1}:\")\n",
    "            print(f\"❓ Query: {result['query']}\")\n",
    "            print(f\"🎯 Expected: {result['expected_answer']}\")\n",
    "            print(f\"🤖 RAG Answer: {result['actual_answer']}\")\n",
    "            print(f\"📊 Word Overlap: {result['word_overlap_score']:.3f}\")\n",
    "            print(f\"✅ Contains Expected: {result['contains_expected']}\")\n",
    "            print(f\"📂 Category: {result.get('category', 'unknown')}\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"❌ No successful results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with LLM as Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing LLM Judge...\n",
      "✅ LLM Judge Test Results:\n",
      "  llm_judge_relevance: 1.0\n",
      "  llm_judge_completeness: 1.0\n",
      "  llm_judge_intent: 1.0\n",
      "  llm_judge_correctness: 1.0\n",
      "  llm_judge_average: 1.0\n",
      "  llm_judge_raw_response: {\n",
      "  \"relevance\": 1.0,\n",
      "  \"completeness\": 1.0,\n",
      "  \"intent\": 1.0,\n",
      "  \"correctness\": 1.0\n",
      "}\n",
      "  llm_judge_error: None\n",
      "  llm_judge_attempts: 1\n",
      "🎉 LLM Judge working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test LLM Judge functionality first\n",
    "from evaluations.utils.llm_judge import LLMJudge\n",
    "\n",
    "print(\"🧪 Testing LLM Judge...\")\n",
    "\n",
    "# Initialize judge\n",
    "judge = LLMJudge()\n",
    "\n",
    "# Quick test\n",
    "test_result = judge.evaluate_response(\n",
    "    query=\"How many bedrooms does this place have?\",\n",
    "    expected_answer=\"2\", \n",
    "    actual_answer=\"This listing features 2 comfortable bedrooms with queen-sized beds.\"\n",
    ")\n",
    "\n",
    "print(\"✅ LLM Judge Test Results:\")\n",
    "for key, value in test_result.items():\n",
    "    if key.startswith('llm_judge'):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if test_result.get('llm_judge_error'):\n",
    "    print(\"❌ LLM Judge has errors - check your Ollama setup\")\n",
    "else:\n",
    "    print(\"🎉 LLM Judge working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Running evaluation WITH LLM Judge (this will take longer)...\n",
      "Loading evaluation dataset...\n",
      "Evaluating 10 queries...\n",
      "🧠 LLM Judge evaluation enabled (this will take longer)\n",
      "Processing query 1/10: What type of property is this?...\n",
      "  🧠 Running LLM judge for query 1...\n",
      "Processing query 2/10: How many bedrooms does this place have?...\n",
      "  🧠 Running LLM judge for query 2...\n",
      "Processing query 3/10: What amenities are available?...\n",
      "  🧠 Running LLM judge for query 3...\n",
      "Processing query 4/10: What neighborhood is this located in?...\n",
      "  🧠 Running LLM judge for query 4...\n",
      "Processing query 5/10: How many people can this accommodate?...\n",
      "  🧠 Running LLM judge for query 5...\n",
      "Processing query 6/10: What is the room type?...\n",
      "  🧠 Running LLM judge for query 6...\n",
      "Processing query 7/10: What type of property is this?...\n",
      "  🧠 Running LLM judge for query 7...\n",
      "Processing query 8/10: How many bedrooms does this place have?...\n",
      "  🧠 Running LLM judge for query 8...\n",
      "Processing query 9/10: What amenities are available?...\n",
      "  🧠 Running LLM judge for query 9...\n",
      "Processing query 10/10: What neighborhood is this located in?...\n",
      "  🧠 Running LLM judge for query 10...\n",
      "\n",
      "Evaluation completed!\n",
      "Results saved to: evaluations/results/rag_evaluation/rag_evaluation_with_judge_20250529_215549.json\n",
      "Summary statistics:\n",
      "  total_queries: 10\n",
      "  successful_evaluations: 10\n",
      "  error_rate: 0.000\n",
      "  avg_response_length: 376.300\n",
      "  contains_expected_rate: 0.000\n",
      "  avg_word_overlap_score: 0.204\n",
      "  error_response_rate: 0.000\n",
      "  empty_response_rate: 0.000\n",
      "  llm_judge_success_rate: 1.000\n",
      "  avg_llm_judge_relevance: 0.840\n",
      "  avg_llm_judge_completeness: 0.660\n",
      "  avg_llm_judge_intent: 0.810\n",
      "  avg_llm_judge_correctness: 0.900\n",
      "  avg_llm_judge_overall: 0.802\n",
      "✅ LLM Judge evaluation completed!\n",
      "📊 Results saved to: evaluations/results/rag_evaluation/rag_evaluation_with_judge_20250529_215549.json\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with LLM judge enabled (small sample first)\n",
    "if chunks_file and queries_file:\n",
    "    print(\"🧠 Running evaluation WITH LLM Judge (this will take longer)...\")\n",
    "    \n",
    "    judge_results_file = evaluator.run_evaluation(\n",
    "        chunks_file=chunks_file,\n",
    "        queries_file=queries_file,\n",
    "        chunk_size=100,\n",
    "        max_queries=10,  # Start small for testing\n",
    "        use_llm_judge=True  # 🧠 Enable LLM judge!\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ LLM Judge evaluation completed!\")\n",
    "    print(f\"📊 Results saved to: {judge_results_file}\")\n",
    "else:\n",
    "    print(\"❌ Need dataset files first\")\n",
    "    judge_results_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM JUDGE QUICK SUMMARY\n",
      "------------------------------\n",
      "Success Rate: 100.0%\n",
      "Relevance:    0.840\n",
      "Completeness: 0.660\n",
      "Intent:       0.810\n",
      "Correctness:  0.900\n",
      "Overall:      0.802\n"
     ]
    }
   ],
   "source": [
    "# Quick summary for just the averages\n",
    "def quick_judge_summary(results_file_path):\n",
    "    \"\"\"Quick summary of just the average scores.\"\"\"\n",
    "    \n",
    "    with open(results_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    summary = data['summary']\n",
    "    \n",
    "    print(\"🧠 LLM JUDGE QUICK SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if summary.get('llm_judge_success_rate'):\n",
    "        print(f\"Success Rate: {summary['llm_judge_success_rate']:.1%}\")\n",
    "        print(f\"Relevance:    {summary['avg_llm_judge_relevance']:.3f}\")\n",
    "        print(f\"Completeness: {summary['avg_llm_judge_completeness']:.3f}\")\n",
    "        print(f\"Intent:       {summary['avg_llm_judge_intent']:.3f}\")\n",
    "        print(f\"Correctness:  {summary['avg_llm_judge_correctness']:.3f}\")\n",
    "        print(f\"Overall:      {summary['avg_llm_judge_overall']:.3f}\")\n",
    "    else:\n",
    "        print(\"No LLM judge results available\")\n",
    "\n",
    "# Use it\n",
    "if 'judge_results_file' in locals() and judge_results_file:\n",
    "    quick_judge_summary(judge_results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Testing Phi-4 Judge...\n",
      "✅ Phi-4 Judge initialized\n",
      "✅ Phi-4 Judge Test Results:\n",
      "  phi4_judge_relevance: 1.0\n",
      "  phi4_judge_completeness: 0.8\n",
      "  phi4_judge_intent: 1.0\n",
      "  phi4_judge_correctness: 1.0\n",
      "  phi4_judge_average: 0.95\n",
      "  phi4_judge_raw_response: ```json\n",
      "{\n",
      "  \"relevance\": 1.0,\n",
      "  \"completeness\": 0.8,\n",
      "  \"intent\": 1.0,\n",
      "  \"correctness\": 1.0\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "- **Relevance (1.0):** The actual answer directly addresses the question about the number of bedrooms, making it highly relevant.\n",
      "\n",
      "- **Completeness (0.8):** While the answer provides additional details about the type and size of beds, which is useful, it goes slightly beyond what was minimally required to answer the specific question. However, this extra information can be seen as enhancing the completeness in a practical context.\n",
      "\n",
      "- **Intent (1.0):** The user's intent was to know the number of bedrooms, and the answer fulfills this need by confirming there are 2 bedrooms.\n",
      "\n",
      "- **Correctness (1.0):** The factual accuracy is maintained as the listing indeed has 2 bedrooms, aligning with the expected answer.\n",
      "  phi4_judge_error: None\n",
      "  phi4_judge_attempts: 1\n",
      "  phi4_judge_model: phi4\n",
      "🎉 Phi-4 judge working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test Phi-4 Judge functionality\n",
    "from evaluations.utils.llm_judge_phi4 import LLMJudgePhi4\n",
    "\n",
    "print(\"🔬 Testing Phi-4 Judge...\")\n",
    "\n",
    "try:\n",
    "    # Initialize judge\n",
    "    phi4_judge = LLMJudgePhi4()\n",
    "    print(\"✅ Phi-4 Judge initialized\")\n",
    "\n",
    "    # Quick test\n",
    "    phi4_test_result = phi4_judge.evaluate_response(\n",
    "        query=\"How many bedrooms does this place have?\",\n",
    "        expected_answer=\"2\", \n",
    "        actual_answer=\"This listing features 2 comfortable bedrooms with queen-sized beds.\"\n",
    "    )\n",
    "\n",
    "    print(\"✅ Phi-4 Judge Test Results:\")\n",
    "    for key, value in phi4_test_result.items():\n",
    "        if key.startswith('phi4_judge'):\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    if phi4_test_result.get('phi4_judge_error'):\n",
    "        print(f\"❌ Error: {phi4_test_result['phi4_judge_error']}\")\n",
    "    else:\n",
    "        print(\"🎉 Phi-4 judge working correctly!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"💡 Make sure you have Phi-4 installed:\")\n",
    "    print(\"   ollama pull phi4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Running Phi-4 judge on existing results...\n",
      "  Processing result 1/3...\n",
      "  Processing result 2/3...\n",
      "  Processing result 3/3...\n",
      "✅ Phi-4 average score: 0.733\n",
      "🧠 LLM average score: 0.700\n",
      "📊 Difference: 0.033\n"
     ]
    }
   ],
   "source": [
    "# Use Phi-4 judge on existing results manually\n",
    "if 'judge_results_file' in locals() and judge_results_file:\n",
    "    # Load existing results\n",
    "    with open(judge_results_file, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "    \n",
    "    print(\"🔬 Running Phi-4 judge on existing results...\")\n",
    "    \n",
    "    # Initialize Phi-4 judge\n",
    "    from evaluations.utils.llm_judge_phi4 import LLMJudgePhi4\n",
    "    phi4_judge = LLMJudgePhi4()\n",
    "    \n",
    "    # Run Phi-4 on a few results\n",
    "    results = existing_data['results']\n",
    "    for i, result in enumerate(results[:3]):  # Just first 3\n",
    "        if not result.get('error') and not result.get('is_error'):\n",
    "            print(f\"  Processing result {i+1}/3...\")\n",
    "            \n",
    "            phi4_scores = phi4_judge.evaluate_response(\n",
    "                query=result['query'],\n",
    "                expected_answer=result.get('expected_answer', ''),\n",
    "                actual_answer=result['actual_answer']\n",
    "            )\n",
    "            \n",
    "            # Add phi4 scores to result\n",
    "            result.update(phi4_scores)\n",
    "    \n",
    "    # Show summary\n",
    "    phi4_results = [r for r in results[:3] if r.get('phi4_judge_average') is not None]\n",
    "    if phi4_results:\n",
    "        avg_phi4 = sum(r['phi4_judge_average'] for r in phi4_results) / len(phi4_results)\n",
    "        print(f\"✅ Phi-4 average score: {avg_phi4:.3f}\")\n",
    "        \n",
    "        # Show comparison\n",
    "        avg_llm = sum(r.get('llm_judge_average', 0) for r in phi4_results) / len(phi4_results)\n",
    "        print(f\"🧠 LLM average score: {avg_llm:.3f}\")\n",
    "        print(f\"📊 Difference: {abs(avg_phi4 - avg_llm):.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Running Phi-4 judge on existing results...\n",
      "  Processing result 1/3...\n"
     ]
    }
   ],
   "source": [
    "# Use Phi-4 judge on existing results with detailed summary\n",
    "if 'judge_results_file' in locals() and judge_results_file:\n",
    "    # Load existing results\n",
    "    with open(judge_results_file, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "    \n",
    "    print(\"🔬 Running Phi-4 judge on existing results...\")\n",
    "    \n",
    "    # Initialize Phi-4 judge\n",
    "    from evaluations.utils.llm_judge_phi4 import LLMJudgePhi4\n",
    "    phi4_judge = LLMJudgePhi4()\n",
    "    \n",
    "    # Run Phi-4 on a few results\n",
    "    results = existing_data['results']\n",
    "    phi4_results = []\n",
    "    \n",
    "    for i, result in enumerate(results[:3]):  # Just first 3\n",
    "        if not result.get('error') and not result.get('is_error'):\n",
    "            print(f\"  Processing result {i+1}/3...\")\n",
    "            \n",
    "            phi4_scores = phi4_judge.evaluate_response(\n",
    "                query=result['query'],\n",
    "                expected_answer=result.get('expected_answer', ''),\n",
    "                actual_answer=result['actual_answer']\n",
    "            )\n",
    "            \n",
    "            # Collect successful phi4 results\n",
    "            if phi4_scores.get('phi4_judge_average') is not None:\n",
    "                phi4_results.append(phi4_scores)\n",
    "    \n",
    "    # Calculate detailed summary like the LLM judge\n",
    "    if phi4_results:\n",
    "        print(\"\\n🔬 PHI-4 JUDGE QUICK SUMMARY\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        success_rate = len(phi4_results) / 3  # 3 total attempts\n",
    "        avg_relevance = sum(r['phi4_judge_relevance'] for r in phi4_results) / len(phi4_results)\n",
    "        avg_completeness = sum(r['phi4_judge_completeness'] for r in phi4_results) / len(phi4_results)\n",
    "        avg_intent = sum(r['phi4_judge_intent'] for r in phi4_results) / len(phi4_results)\n",
    "        avg_correctness = sum(r['phi4_judge_correctness'] for r in phi4_results) / len(phi4_results)\n",
    "        avg_overall = sum(r['phi4_judge_average'] for r in phi4_results) / len(phi4_results)\n",
    "        \n",
    "        print(f\"Success Rate: {success_rate:.1%}\")\n",
    "        print(f\"Relevance:    {avg_relevance:.3f}\")\n",
    "        print(f\"Completeness: {avg_completeness:.3f}\")\n",
    "        print(f\"Intent:       {avg_intent:.3f}\")\n",
    "        print(f\"Correctness:  {avg_correctness:.3f}\")\n",
    "        print(f\"Overall:      {avg_overall:.3f}\")\n",
    "    else:\n",
    "        print(\"❌ No successful Phi-4 results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
