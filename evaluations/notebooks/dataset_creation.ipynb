{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Dataset Evaluation Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aus10powell/Documents/Projects/AirAgent/src/utils/nlp_utils.py:13: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=OLLAMA_MODEL, temperature=DEFAULT_TEMPERATURE)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append(os.path.abspath('.'))  # Add project root\n",
    "# sys.path.append(os.path.abspath('./src'))  # Add src directory\n",
    "\n",
    "# Go up 2 directories to reach project root\n",
    "project_root = os.path.abspath('../..')  # Go up 2 levels\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "from evaluations.utils.dataset_generator import DatasetGenerator\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Evaluation Dataset Creation ===\n",
      "\n",
      "1. Initializing Dataset Generator...\n",
      "   Output directory: ../../evaluations/datasets/rag_evaluation\n",
      "\n",
      "2. Creating evaluation dataset...\n",
      "   City: seattle\n",
      "   Number of listings: 100\n",
      "   Chunk size: 100 words\n",
      "Successfully loaded 6770 records from /Users/aus10powell/Documents/Projects/AirRanker/data/seattle/listings.parquet\n",
      "Generated 250 chunks and 596 queries\n",
      "Chunks saved to: ../../evaluations/datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "Queries saved to: ../../evaluations/datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n",
      "\n",
      "   ‚úÖ Dataset created successfully!\n",
      "   üìÅ Chunks file: ../../evaluations/datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "   üìÅ Queries file: ../../evaluations/datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n",
      "\n",
      "3. Exploring generated dataset...\n",
      "   Total chunks: 250\n",
      "   Total queries: 596\n",
      "   Average chunks per listing: 2.5\n",
      "   Average queries per listing: 6.0\n",
      "\n",
      "4. Sample chunk:\n",
      "{\n",
      "  \"chunk_id\": \"seattle_1301535_0\",\n",
      "  \"listing_id\": 1301535,\n",
      "  \"city\": \"seattle\",\n",
      "  \"chunk_text\": \"Listing Name: 2 Huge Private Rooms + Common Area Description: Our basement guest suite has a private entry & all of the amenities to make your Seattle visit enjoyable. You'll be staying in a beautiful, quiet neighborhood that is also within walking distance to many restaurants & 3 blocks from downtown bus lines. Amenities: [\\\"Extra pillows and blankets\\\", \\\"Paid dryer \\\\u2013 In unit\\\", \\\"Conditioner\\\", \\\"Cooking basics\\\", \\\"Bed linens\\\", \\\"Clothing storage: wardrobe and dresser\\\", \\\"Shared patio or balcony\\\", \\\"Coffee maker\\\", \\\"Room-darkening shades\\\", \\\"Smart lock\\\", \\\"Books and reading material\\\", \\\"Portable fans\\\", \\\"Free parking on premises\\\", \\\"Self check-in\\\", \\\"Essentials\\\", \\\"Heating\\\", \\\"Children\\\\u2019s books and\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"listing_name\": \"2 Huge Private Rooms + Common Area\"\n",
      "}\n",
      "\n",
      "5. Sample query:\n",
      "{\n",
      "  \"query\": \"What type of property is this?\",\n",
      "  \"expected_answer\": \"Entire guest suite\",\n",
      "  \"category\": \"property_details\",\n",
      "  \"listing_id\": 1301535,\n",
      "  \"chunks\": [\n",
      "    \"seattle_1301535_0\",\n",
      "    \"seattle_1301535_1\",\n",
      "    \"seattle_1301535_2\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "6. Query category distribution:\n",
      "   property_details: 396 queries\n",
      "   amenities: 100 queries\n",
      "   location: 100 queries\n",
      "\n",
      "7. Chunk length statistics:\n",
      "   Mean length: 80.2 words\n",
      "   Median length: 100.0 words\n",
      "   Min length: 1 words\n",
      "   Max length: 100 words\n",
      "   Target chunk size: 100 words\n",
      "\n",
      "8. Example listing breakdown:\n",
      "   Listing ID: 53127334\n",
      "   Listing Name: Serene City Studio\n",
      "   Number of chunks: 3\n",
      "   Number of queries: 6\n",
      "\n",
      "   First chunk preview:\n",
      "   Listing Name: Serene City Studio Description: Vibrant studio guest house. Every comfort was thought of as this studio was designed (and used primarily for) visiting family. This studio offers 2 queen ...\n",
      "\n",
      "   Sample queries for this listing:\n",
      "   1. Q: What type of property is this?\n",
      "      Expected: Entire guesthouse\n",
      "      Category: property_details\n",
      "   2. Q: How many bedrooms does this place have?\n",
      "      Expected: 1.0\n",
      "      Category: property_details\n",
      "   3. Q: What amenities are available?\n",
      "      Expected: [\"Extra pillows and blankets\", \"Baking sheet\", \"Conditioner\", \"Ping pong table\", \"Children\\u2019s books and toys for ages 0-2 years old and 2-5 years old\", \"Bed linens\", \"Cooking basics\", \"Private backyard \\u2013 Fully fenced\", \"Beach essentials\", \"Kitchen\", \"Convection stainless steel oven\", \"Smart lock\", \"AC - split type ductless system\", \"Induction stove\", \"Books and reading material\", \"Cleaning products\", \"Free parking on premises\", \"Self check-in\", \"Bidet\", \"Essentials\", \"Pack \\u2019n play/Travel crib - always at the listing\", \"Children\\u2019s dinnerware\", \"Bathtub\", \"Laundromat nearby\", \"High chair\", \"Smoke alarm\", \"Iron\", \"Exercise equipment\", \"Coffee\", \"Coffee maker: drip coffee maker, french press, Keurig coffee machine\", \"Shared beach access\", \"Bikes\", \"Fire extinguisher\", \"Dedicated workspace\", \"Carbon monoxide alarm\", \"Central heating\", \"Dishwasher\", \"Exterior security cameras on property\", \"Wine glasses\", \"Dishes and silverware\", \"55 inch HDTV with Roku, DVD player\", \"Free dryer \\u2013 In unit\", \"Board games\", \"Free washer \\u2013 In unit\", \"Hot water\", \"Hangers\", \"Refrigerator\", \"Game console: PS4\", \"Freezer\", \"Private entrance\", \"Hair dryer\", \"Shower gel\", \"Ethernet connection\", \"Toaster\", \"Clothing storage: closet\", \"First aid kit\", \"Private gym in building\", \"Microwave\", \"Body soap\", \"Shampoo\", \"Fast wifi \\u2013 804 Mbps\", \"Blender\"]\n",
      "      Category: amenities\n",
      "\n",
      "=== Dataset Creation Complete! ===\n",
      "You can now use these files with the RAG evaluator:\n",
      "- Chunks: ../../evaluations/datasets/rag_evaluation/chunks_seattle_100listings_20250529_204239.json\n",
      "- Queries: ../../evaluations/datasets/rag_evaluation/queries_seattle_100listings_20250529_204239.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Standalone script to create RAG evaluation dataset.\n",
    "This shows exactly what the Jupyter notebook does to generate the evaluation data.\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Create and explore a RAG evaluation dataset.\"\"\"\n",
    "\n",
    "    print(\"=== RAG Evaluation Dataset Creation ===\\n\")\n",
    "\n",
    "    # 1. Initialize Dataset Generator\n",
    "    print(\"1. Initializing Dataset Generator...\")\n",
    "    generator = DatasetGenerator(output_dir=\"../../evaluations/datasets/rag_evaluation\")\n",
    "    print(f\"   Output directory: {generator.output_dir}\")\n",
    "\n",
    "    # 2. Create Evaluation Dataset\n",
    "    print(\"\\n2. Creating evaluation dataset...\")\n",
    "    city = \"seattle\"\n",
    "    num_listings = 100\n",
    "    chunk_size = 100\n",
    "\n",
    "    print(f\"   City: {city}\")\n",
    "    print(f\"   Number of listings: {num_listings}\")\n",
    "    print(f\"   Chunk size: {chunk_size} words\")\n",
    "\n",
    "    try:\n",
    "        chunks_file, queries_file = generator.create_rag_evaluation_dataset(\n",
    "            city=city,\n",
    "            num_listings=num_listings,\n",
    "            chunk_size=chunk_size,\n",
    "            seed=42,  # For reproducibility\n",
    "        )\n",
    "\n",
    "        print(f\"\\n   ‚úÖ Dataset created successfully!\")\n",
    "        print(f\"   üìÅ Chunks file: {chunks_file}\")\n",
    "        print(f\"   üìÅ Queries file: {queries_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Load and Explore the Generated Dataset\n",
    "    print(\"\\n3. Exploring generated dataset...\")\n",
    "    chunks, queries = generator.load_evaluation_dataset(chunks_file, queries_file)\n",
    "\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Total queries: {len(queries)}\")\n",
    "    print(f\"   Average chunks per listing: {len(chunks) / num_listings:.1f}\")\n",
    "    print(f\"   Average queries per listing: {len(queries) / num_listings:.1f}\")\n",
    "\n",
    "    # 4. Show Sample Data\n",
    "    print(\"\\n4. Sample chunk:\")\n",
    "    print(json.dumps(chunks[0], indent=2))\n",
    "\n",
    "    print(\"\\n5. Sample query:\")\n",
    "    print(json.dumps(queries[0], indent=2))\n",
    "\n",
    "    # 6. Analyze Query Categories\n",
    "    print(\"\\n6. Query category distribution:\")\n",
    "    categories = [q.get(\"category\", \"unknown\") for q in queries]\n",
    "    category_counts = pd.Series(categories).value_counts()\n",
    "\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"   {category}: {count} queries\")\n",
    "\n",
    "    # 7. Analyze Chunk Statistics\n",
    "    print(\"\\n7. Chunk length statistics:\")\n",
    "    chunk_lengths = [len(chunk[\"chunk_text\"].split()) for chunk in chunks]\n",
    "\n",
    "    print(f\"   Mean length: {pd.Series(chunk_lengths).mean():.1f} words\")\n",
    "    print(f\"   Median length: {pd.Series(chunk_lengths).median():.1f} words\")\n",
    "    print(f\"   Min length: {min(chunk_lengths)} words\")\n",
    "    print(f\"   Max length: {max(chunk_lengths)} words\")\n",
    "    print(f\"   Target chunk size: {chunk_size} words\")\n",
    "\n",
    "    # 8. Show Example Listing with its Chunks and Queries\n",
    "    print(\"\\n8. Example listing breakdown:\")\n",
    "\n",
    "    # Get a random listing ID\n",
    "    import random\n",
    "\n",
    "    sample_listing_id = random.choice([q[\"listing_id\"] for q in queries])\n",
    "\n",
    "    # Find chunks for this listing\n",
    "    listing_chunks = [c for c in chunks if c[\"listing_id\"] == sample_listing_id]\n",
    "    listing_queries = [q for q in queries if q[\"listing_id\"] == sample_listing_id]\n",
    "\n",
    "    print(f\"   Listing ID: {sample_listing_id}\")\n",
    "    print(\n",
    "        f\"   Listing Name: {listing_chunks[0]['listing_name'] if listing_chunks else 'Unknown'}\"\n",
    "    )\n",
    "    print(f\"   Number of chunks: {len(listing_chunks)}\")\n",
    "    print(f\"   Number of queries: {len(listing_queries)}\")\n",
    "\n",
    "    if listing_chunks:\n",
    "        print(f\"\\n   First chunk preview:\")\n",
    "        print(f\"   {listing_chunks[0]['chunk_text'][:200]}...\")\n",
    "\n",
    "    print(f\"\\n   Sample queries for this listing:\")\n",
    "    for i, query in enumerate(listing_queries[:3]):\n",
    "        print(f\"   {i+1}. Q: {query['query']}\")\n",
    "        print(f\"      Expected: {query['expected_answer']}\")\n",
    "        print(f\"      Category: {query['category']}\")\n",
    "\n",
    "    print(f\"\\n=== Dataset Creation Complete! ===\")\n",
    "    print(f\"You can now use these files with the RAG evaluator:\")\n",
    "    print(f\"- Chunks: {chunks_file}\")\n",
    "    print(f\"- Queries: {queries_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
